import streamlit as st
import torch
import numpy as np
from transformers import AutoTokenizer, RobertaConfig, RobertaForSequenceClassification
from tensorflow.keras.preprocessing.sequence import pad_sequences # V·∫´n d√πng t·ª´ code g·ªëc
import re
import os
import requests # ƒê·ªÉ t·∫£i file t·ª´ URL
from tqdm import tqdm # ƒê·ªÉ hi·ªÉn th·ªã thanh ti·∫øn tr√¨nh (t√πy ch·ªçn)

# --- C·∫•u h√¨nh Model v√† C√°c h·∫±ng s·ªë ---
MODEL_NAME_FOR_TOKENIZER_AND_ARCHITECTURE = "vinai/phobert-base"
# URL tr·ª±c ti·∫øp ƒë·ªÉ t·∫£i file .pt t·ª´ Hugging Face (s·ª≠ d·ª•ng link "raw")
MODEL_URL = "https://huggingface.co/binhkhang3169/phoBERTtoxic/raw/main/model_best_valacc.pt"
MODEL_FILENAME = "model_best_valacc.pt" # T√™n file model s·∫Ω l∆∞u c·ª•c b·ªô

MAX_LEN = 256  # Ph·∫£i kh·ªõp v·ªõi MAX_LEN khi training
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- C√°c h√†m Helper (Gi·ªØ nguy√™n t·ª´ code g·ªëc) ---
def make_mask(batch_ids):
    batch_mask = []
    for ids in batch_ids:
        mask = [int(token_id > 0) for token_id in ids]
        batch_mask.append(mask)
    return torch.tensor(batch_mask)

def clean_text(text):
    # 1. Lo·∫°i b·ªè c√°c k√Ω t·ª± kh√¥ng mong mu·ªën (gi·ªØ l·∫°i ch·ªØ c√°i, s·ªë v√† ti·∫øng Vi·ªát c√≥ d·∫•u)
    text = re.sub(r"[^\w\s√Ä√Å√Ç√É√à√â√ä√å√ç√í√ì√î√ï√ô√öƒÇƒêƒ®≈®∆†√†√°√¢√£√®√©√™√¨√≠√≤√≥√¥√µ√π√∫ƒÉƒëƒ©≈©∆°∆ØƒÇ·∫†·∫¢·∫§·∫¶·∫®·∫™·∫¨·∫Æ·∫∞·∫≤·∫¥·∫∂·∫∏·∫∫·∫º·ªÄ·ªÄ·ªÇ∆∞ƒÉ·∫°·∫£·∫•·∫ß·∫©·∫´·∫≠·∫Ø·∫±·∫≥·∫µ·∫∑·∫π·∫ª·∫Ω·ªÅ·ªÅ·ªÉ·∫ø·ªÑ·ªÜ·ªà·ªä·ªå·ªé·ªê·ªí·ªî·ªñ·ªò·ªö·ªú·ªû·ª†·ª¢·ª§·ª¶·ª®·ª™·ªÖ·ªá·ªâ·ªã·ªç·ªè·ªë·ªì·ªï·ªó·ªô·ªõ·ªù·ªü·ª°·ª£·ª•·ªß·ª©·ª´·ª¨·ªÆ·ª∞·ª≥·ªµ·ª∑·ªπ√Ω√ù·ª≤·ª¥·ª∂·ª∏]", ' ', text)
    
    # 2. Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng tr∆∞·ªõc khi lo·∫°i b·ªè ƒë·∫°i t·ª´ ƒë·ªÉ b·∫Øt t·∫•t c·∫£ c√°c tr∆∞·ªùng h·ª£p (v√≠ d·ª•: "B·∫°n", "b·∫°n")
    text = text.lower()

    # 3. Danh s√°ch c√°c ƒë·∫°i t·ª´ c·∫ßn lo·∫°i b·ªè
    pronouns_to_remove = [
        "b·∫°n", "t√¥i", "c·∫≠u", "t·ªõ", "m√¨nh",
        # B·∫°n c√≥ th·ªÉ th√™m c√°c bi·∫øn th·ªÉ ho·∫∑c t·ª´ kh√°c n·∫øu mu·ªën
        # V√≠ d·ª•: "tao", "m√†y", "ch√∫ng ta", "ch√∫ng t√¥i", "ch√∫ng n√≥"
    ]
    
    # T·∫°o pattern regex ƒë·ªÉ kh·ªõp ch√≠nh x√°c c√°c t·ª´ n√†y (d√πng \b ƒë·ªÉ kh·ªõp ranh gi·ªõi t·ª´)
    # V√≠ d·ª•: \bb·∫°n\b s·∫Ω kh·ªõp "b·∫°n" nh∆∞ng kh√¥ng kh·ªõp "b·∫°nhi·ªÅn"
    # D√πng re.escape ƒë·ªÉ ƒë·∫£m b·∫£o c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát trong t·ª´ (n·∫øu c√≥) ƒë∆∞·ª£c x·ª≠ l√Ω ƒë√∫ng
    pattern = r'\b(' + '|'.join(re.escape(pronoun) for pronoun in pronouns_to_remove) + r')\b'
    text = re.sub(pattern, '', text, flags=re.IGNORECASE) # flags=re.IGNORECASE c√≥ th·ªÉ kh√¥ng c·∫ßn n·∫øu ƒë√£ lower() ·ªü tr√™n

    # 4. Chu·∫©n h√≥a kho·∫£ng tr·∫Øng (lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a)
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text # Kh√¥ng c·∫ßn .lower() n·ªØa v√¨ ƒë√£ l√†m ·ªü b∆∞·ªõc 2

# --- ƒê·ªãnh nghƒ©a Model (Gi·ªØ nguy√™n t·ª´ code g·ªëc) ---
class BERTClassifier(torch.nn.Module):
    def __init__(self, num_labels=2, model_name=MODEL_NAME_FOR_TOKENIZER_AND_ARCHITECTURE):
        super(BERTClassifier, self).__init__()
        bert_classifier_config = RobertaConfig.from_pretrained(
            model_name,
            num_labels=num_labels,
            output_hidden_states=False,
        )
        self.bert_classifier = RobertaForSequenceClassification.from_pretrained(
            model_name,
            config=bert_classifier_config
        )

    def forward(self, input_ids, attention_mask, labels=None):
        output = self.bert_classifier(
            input_ids=input_ids,
            token_type_ids=None,
            attention_mask=attention_mask,
            labels=labels
        )
        return output

# --- H√†m t·∫£i file t·ª´ URL v·ªõi thanh ti·∫øn tr√¨nh ---
def download_file_from_url(url, destination_path, chunk_size=8192):
    try:
        response = requests.get(url, stream=True)
        response.raise_for_status()
        total_size = int(response.headers.get('content-length', 0))
        
        # S·ª≠ d·ª•ng st.empty() ƒë·ªÉ t·∫°o m·ªôt placeholder cho progress bar
        progress_bar_placeholder = st.empty()
        status_text_placeholder = st.empty()

        downloaded_size = 0
        status_text_placeholder.info(f"Downloading {os.path.basename(destination_path)}...")

        with open(destination_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=chunk_size):
                if chunk:
                    f.write(chunk)
                    downloaded_size += len(chunk)
                    if total_size > 0:
                        progress = min(int((downloaded_size / total_size) * 100), 100)
                        progress_bar_placeholder.progress(progress)
        
        status_text_placeholder.success(f"Download complete: {os.path.basename(destination_path)}")
        progress_bar_placeholder.empty() # X√≥a progress bar sau khi ho√†n t·∫•t
        return True
    except requests.exceptions.RequestException as e:
        st.error(f"Error downloading model: {e}")
        if os.path.exists(destination_path):
            os.remove(destination_path)
        return False
    except Exception as e:
        st.error(f"An unexpected error occurred during download: {e}")
        if os.path.exists(destination_path):
            os.remove(destination_path)
        return False

# --- H√†m t·∫£i Model v√† Tokenizer (Cached) ---
@st.cache_resource  # Cache resource n√†y ƒë·ªÉ kh√¥ng ph·∫£i t·∫£i l·∫°i model m·ªói l·∫ßn
def load_model_and_tokenizer():
    local_model_path = MODEL_FILENAME

    if not os.path.exists(local_model_path):
        st.info(f"Model file '{local_model_path}' not found locally. Attempting to download...")
        if not MODEL_URL:
            st.error("MODEL_URL is not configured.")
            st.stop()
        if not download_file_from_url(MODEL_URL, local_model_path):
            st.error("Failed to download model. Please check the URL and network connection.")
            st.stop()
    
    try:
        # T·∫£i Tokenizer
        phobert_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_FOR_TOKENIZER_AND_ARCHITECTURE, use_fast=False)

        # Kh·ªüi t·∫°o c·∫•u tr√∫c model
        phobert_model_structure = BERTClassifier(num_labels=2, model_name=MODEL_NAME_FOR_TOKENIZER_AND_ARCHITECTURE)

        # T·∫£i tr·ªçng s·ªë ƒë√£ train
        checkpoint = torch.load(local_model_path, map_location=DEVICE)
        
        # Ki·ªÉm tra ƒë·ªãnh d·∫°ng checkpoint
        if 'model_state_dict' in checkpoint:
            phobert_model_structure.load_state_dict(checkpoint['model_state_dict'])
        elif isinstance(checkpoint, dict) and all(isinstance(k, str) for k in checkpoint.keys()): # N·∫øu l√† state_dict tr·ª±c ti·∫øp
            phobert_model_structure.load_state_dict(checkpoint)
        else:
            st.error("Model checkpoint format not recognized. Expected a state_dict or a dict with 'model_state_dict'.")
            st.stop()
            
        phobert_model = phobert_model_structure.to(DEVICE)
        phobert_model.eval() # Chuy·ªÉn sang ch·∫ø ƒë·ªô ƒë√°nh gi√°
        
        # st.success(f"Model and tokenizer loaded successfully on {DEVICE}!") # Th√¥ng b√°o khi t·∫£i xong (c√≥ th·ªÉ ·∫©n ƒëi)
        return phobert_model, phobert_tokenizer
    except Exception as e:
        st.error(f"Error loading model or tokenizer: {e}")
        # C√¢n nh·∫Øc x√≥a file n·∫øu t·∫£i v·ªÅ m√† load l·ªói ƒë·ªÉ l·∫ßn sau t·∫£i l·∫°i
        # if os.path.exists(local_model_path) and MODEL_URL:
        #     os.remove(local_model_path)
        #     st.warning(f"Removed potentially corrupted local model file '{local_model_path}'. Please try reloading.")
        st.stop()
        return None, None

# --- T·∫£i model v√† tokenizer khi ·ª©ng d·ª•ng kh·ªüi ch·∫°y ---
# Th√¥ng b√°o cho ng∆∞·ªùi d√πng bi·∫øt qu√° tr√¨nh t·∫£i model c√≥ th·ªÉ m·∫•t th·ªùi gian
with st.spinner(f"Loading model from {MODEL_NAME_FOR_TOKENIZER_AND_ARCHITECTURE} and weights... This may take a moment."):
    phobert_model, phobert_tokenizer = load_model_and_tokenizer()

# --- Giao di·ªán ng∆∞·ªùi d√πng Streamlit ---
st.set_page_config(page_title="Toxic Comment Detector", layout="centered")
st.title("üí¨ Toxic Comment Detector")
st.markdown("Enter Vietnamese text below to check if it's toxic or non-toxic.")

# S·ª≠ d·ª•ng session state ƒë·ªÉ gi·ªØ l·∫°i input v√† k·∫øt qu·∫£ sau m·ªói l·∫ßn t∆∞∆°ng t√°c
if 'text_input' not in st.session_state:
    st.session_state.text_input = ""
if 'prediction_result' not in st.session_state:
    st.session_state.prediction_result = None
if 'confidence_score' not in st.session_state:
    st.session_state.confidence_score = None
if 'submitted_text' not in st.session_state:
    st.session_state.submitted_text = ""


# Input text area
user_input = st.text_area("Enter your comment here:", 
                          value=st.session_state.text_input, 
                          height=150, 
                          placeholder="Nh·∫≠p b√¨nh lu·∫≠n ti·∫øng Vi·ªát c·ªßa b·∫°n ·ªü ƒë√¢y...",
                          key="main_text_input")

if st.button("üîç Check Toxicity", use_container_width=True, type="primary"):
    if phobert_model is None or phobert_tokenizer is None:
        st.error("Model is not loaded. Please check for errors during startup.")
    elif not user_input.strip():
        st.warning("Please enter some text to analyze.")
        st.session_state.prediction_result = None # X√≥a k·∫øt qu·∫£ c≈© n·∫øu input r·ªóng
        st.session_state.confidence_score = None
        st.session_state.submitted_text = ""
    else:
        st.session_state.text_input = user_input # L∆∞u input hi·ªán t·∫°i
        st.session_state.submitted_text = user_input # L∆∞u input ƒë√£ submit ƒë·ªÉ hi·ªÉn th·ªã

        with st.spinner("Analyzing..."):
            cleaned_text = clean_text(user_input)

            encoded_text = phobert_tokenizer.encode(cleaned_text, truncation=True, max_length=MAX_LEN, add_special_tokens=True)
            ids_padded = pad_sequences([encoded_text], maxlen=MAX_LEN, dtype="long",
                                       value=phobert_tokenizer.pad_token_id if phobert_tokenizer.pad_token_id is not None else 0,
                                       truncating="post", padding="post")

            input_ids_tensor = torch.tensor(ids_padded).to(DEVICE)
            attention_mask_tensor = make_mask(ids_padded).to(DEVICE)

            with torch.no_grad():
                outputs = phobert_model(input_ids_tensor, attention_mask=attention_mask_tensor)

            logits = outputs[0] if isinstance(outputs, tuple) else outputs.logits
            probabilities = torch.nn.functional.softmax(logits, dim=1)
            confidence, predicted_class_idx = torch.max(probabilities, dim=1)

            predicted_label = "Toxic ‚ò†Ô∏è" if predicted_class_idx.item() == 1 else "Non-toxic üëç"
            
            st.session_state.prediction_result = predicted_label
            st.session_state.confidence_score = confidence.item()

# Hi·ªÉn th·ªã k·∫øt qu·∫£
if st.session_state.prediction_result and st.session_state.submitted_text:
    st.markdown("---")
    st.subheader("üìù Result:")
    
    # Hi·ªÉn th·ªã l·∫°i text ƒë√£ nh·∫≠p
    st.markdown(f"**Your input:**")
    st.markdown(f"> _{st.session_state.submitted_text}_")

    # Hi·ªÉn th·ªã k·∫øt qu·∫£
    if st.session_state.prediction_result == "Toxic ‚ò†Ô∏è":
        st.error(f"**Prediction:** {st.session_state.prediction_result}")
    else:
        st.success(f"**Prediction:** {st.session_state.prediction_result}")
    
    st.info(f"**Confidence:** {st.session_state.confidence_score*100:.2f}%")

st.markdown("---")
st.markdown("Powered by [Streamlit](https://streamlit.io) | Model based on [PhoBERT](https://huggingface.co/vinai/phobert-base) | Weights by [binhkhang3169](https://huggingface.co/binhkhang3169)")